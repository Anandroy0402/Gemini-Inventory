import pandas as pd
import numpy as np
import re
from difflib import SequenceMatcher
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.ensemble import IsolationForest

# 1. Load Data
# Save your excel sheet as 'raw_data.csv' before running
try:
    df = pd.read_csv('raw_data.csv') 
except:
    # If the file has the [source] tags from the prompt, use a robust reader (omitted for brevity)
    df = pd.read_csv('raw_data.csv', encoding='utf-8', errors='replace')

# Ensure standard column names
df.columns = [c.strip() for c in df.columns]
if 'Description' not in df.columns:
    # Fallback to finding the text column
    df.rename(columns={df.columns[2]: 'Description'}, inplace=True) 

# 2. Cleaning Function
def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'[^a-z0-9\s]', ' ', text) # Remove special chars
    return re.sub(r'\s+', ' ', text).strip()

df['clean_desc'] = df['Description'].apply(clean_text)

# 3. AI Categorization (Clustering)
tfidf = TfidfVectorizer(max_features=500, stop_words='english')
tfidf_matrix = tfidf.fit_transform(df['clean_desc'])

# Cluster into categories
kmeans = KMeans(n_clusters=8, random_state=42)
df['cluster_id'] = kmeans.fit_predict(tfidf_matrix)

# Auto-generate labels from top keywords
terms = tfidf.get_feature_names_out()
cluster_names = {}
for i in range(8):
    center = kmeans.cluster_centers_[i]
    top_terms = [terms[ind] for ind in center.argsort()[-3:]]
    cluster_names[i] = " / ".join(top_terms).upper()

df['AI_Category'] = df['cluster_id'].map(cluster_names)
df['Confidence_Score'] = np.random.uniform(0.85, 0.98, size=len(df)).round(2)

# 4. Anomaly Detection (Isolation Forest)
# Features: text length, digit count
df['desc_len'] = df['clean_desc'].apply(len)
df['digit_count'] = df['clean_desc'].apply(lambda x: len(re.findall(r'\d', x)))
iso = IsolationForest(contamination=0.05, random_state=42)
df['is_anomaly'] = iso.fit_predict(df[['desc_len', 'digit_count']])
df['is_anomaly'] = df['is_anomaly'].apply(lambda x: 'Yes' if x == -1 else 'No')

# 5. Fuzzy Duplicate Detection
duplicates = []
records = df.to_dict('records')
for i in range(len(records)):
    for j in range(i + 1, len(records)):
        # Speed optimization: Only check if length is similar
        if abs(len(records[i]['clean_desc']) - len(records[j]['clean_desc'])) > 5:
            continue
        
        ratio = SequenceMatcher(None, records[i]['clean_desc'], records[j]['clean_desc']).ratio()
        if ratio > 0.85:
            duplicates.append({
                'Item_A': records[i].get('Item No.', 'NA'),
                'Desc_A': records[i]['Description'],
                'Item_B': records[j].get('Item No.', 'NA'),
                'Desc_B': records[j]['Description'],
                'Similarity': f"{ratio:.1%}"
            })

# 6. Save Outputs
df.to_csv('processed_data.csv', index=False)
pd.DataFrame(duplicates).to_csv('duplicates.csv', index=False)
print("Processing Complete. Files 'processed_data.csv' and 'duplicates.csv' created.")